docker pull aminbenali/bigdata:VCF


docker run -itd -v ~/Documents/hadoop_project/:/shared_volume --net=hadoop -p 9870:9870 -p 8088:8088 -p 7077:7077 -p 19888:19888 -p 8080:8080 -p 9200:9200 -p 5601:5601 -p 5000:5000 --name hadoop-master-ALL --hostname hadoop-master aminbenali/bigdata:VCF

#we are using the same slaves dyal l3alami	
docker start hadoop-master-ALL hadoop-slave1 hadoop-slave2

par defaut : docker exec -it hadoop-master-ALL bash  ==> /root
but everything is in : /usr/local

to run Hadoop, use tp d alami
to use spark, the files you need are in /usr/local/spark/bin
to run kafka, the files you need are in /usr/local/kafka (/config pour les configurations et /bin pour les scripts)

to run elastic and kibana use : 

sudo -u elk_user ./usr/local/elasticsearch/bin/elasticsearch
# wait for elastic and then
sudo -u elk_user ./usr/local/elasticsearch/bin/kibana

python and pip3 are installed as well as mlflow

airflow can used by running the command : airflow webserver ( or sudo -u elk_user airflow webserver in case can't run as root error) ==> accessible on port 8080 and username: airflow , password : airflow
yet it still needs some configurations, ila hin lihtiyaj.


